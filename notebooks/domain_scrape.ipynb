{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in imports\n",
    "import re\n",
    "import pandas as pd\n",
    "from json import dump\n",
    "from time import sleep\n",
    "from datetime import date\n",
    "import requests\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# user packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in list of postcodes in VIC\n",
    "file = open('../data/raw/postcodes.txt', 'r')\n",
    "\n",
    "data = file.read()\n",
    "\n",
    "postcodes = data.replace('\\n', ' ').split(\" \")\n",
    "\n",
    "file.close()\n",
    "\n",
    "postcodes = postcodes[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "RENT_LINK1 = f'/rent/?postcode='\n",
    "RENT_LINK2 = f'&sort=default-desc&page='\n",
    "N_PAGES = range(1, 51)\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin code\n",
    "url_links = []\n",
    "property_metadata = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of urls to visit\n",
    "for code in postcodes:\n",
    "    rent_url_incomplete = BASE_URL + RENT_LINK1 + code\n",
    "    for page in N_PAGES:\n",
    "        rent_url = rent_url_incomplete + RENT_LINK2 + str(page)\n",
    "        \n",
    "        bs_object_rent = BeautifulSoup(requests.get(\n",
    "                                                    rent_url, \n",
    "                                                    headers=headers).text, \n",
    "                                                    \"html.parser\")\n",
    "        \n",
    "\n",
    "        # find all href (a) tags that are from the rent_url website.\n",
    "        index_links = bs_object_rent \\\n",
    "            .findAll(\n",
    "                \"a\")\n",
    "\n",
    "        for link in index_links:\n",
    "            # if its a property address, add it to the list\n",
    "            if 'address' in link['class']:\n",
    "                url_links.append(link['href'])\n",
    "        \n",
    "# Remove duplicate links\n",
    "url_links = list(set(url_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save urls to url.txt for storage if needed\n",
    "\n",
    "#with open('../data/raw/urls.txt', 'w') as f:\n",
    "#    for line in url_links:\n",
    "#        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in list of urls from url.txt if needed\n",
    "\n",
    "#file = open('../data/raw/urls.txt', 'r')\n",
    "#data = file.read()\n",
    "#url_links = data.replace('\\n', ' ').split(\" \")\n",
    "#file.close()\n",
    "#url_links = url_links[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty csv file\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(f'../data/raw/property_data{str(date.today())[4:]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSIGHTS_AGEGROUP = ['under 20', '20 - 39', '40 - 59', '60+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each url, scrape some basic metadata\n",
    "for property_url in url_links:\n",
    "    property_metadata = defaultdict(dict)\n",
    "    \n",
    "    bs_object_rent = BeautifulSoup(requests.get(\n",
    "                                                property_url, \n",
    "                                                headers=headers).text, \n",
    "                                                \"html.parser\")\n",
    "\n",
    "    # looks for the header class to get property name\n",
    "    try:\n",
    "        property_metadata[property_url]['name'] = bs_object_rent \\\n",
    "            .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "            .text\n",
    "    except:\n",
    "        property_metadata[property_url]['name'] = None\n",
    "\n",
    "    # looks for the div containing a summary title for cost\n",
    "    try:\n",
    "        property_metadata[property_url]['cost_text'] = bs_object_rent \\\n",
    "            .find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}) \\\n",
    "            .text\n",
    "    except:\n",
    "        property_metadata[property_url]['cost_text'] = None\n",
    "\n",
    "    # Find text for property type\n",
    "    try:\n",
    "        property_metadata[property_url]['property_type'] = bs_object_rent \\\n",
    "            .find(\"div\", {\"data-testid\": \"listing-summary-property-type\"}) \\\n",
    "            .findAll(\"span\", {\"class\": \"css-in3yi3\"})[0]\n",
    "    except:\n",
    "        property_metadata[property_url]['property_type'] = None\n",
    "    \n",
    "    # Record any additional information provided\n",
    "    try:\n",
    "        property_metadata[property_url]['extra'] = bs_object_rent \\\n",
    "            .find(\"ul\", {\"data-testid\": \"listing-summary-strip\"})\n",
    "    except:\n",
    "        property_metadata[property_url]['extra'] = None\n",
    "    \n",
    "    # Extract coordinates from the hyperlink provided\n",
    "    try:\n",
    "        property_metadata[property_url]['coordinates'] = [\n",
    "            float(coord) for coord in re.findall(\n",
    "                r'destination=([-\\s,\\d\\.]+)',\n",
    "                bs_object_rent \\\n",
    "                    .find(\n",
    "                        \"a\",\n",
    "                        {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "                    ) \\\n",
    "                    .attrs['href']\n",
    "            )[0].split(',')\n",
    "        ]\n",
    "    except:\n",
    "        property_metadata[property_url]['coordinates'] = None\n",
    "\n",
    "    # Extract data for number of each room type\n",
    "    try:\n",
    "        property_metadata[property_url]['rooms'] = [\n",
    "            re.findall(r'\\d\\s[A-Za-z]+', feature.text) for feature in bs_object_rent \\\n",
    "                .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "                .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "        ]\n",
    "    except:\n",
    "        property_metadata[property_url]['rooms'] = None\n",
    "\n",
    "    # Scrape the description title\n",
    "    try:\n",
    "        property_metadata[property_url]['desc_title'] = bs_object_rent \\\n",
    "            .findAll(\"h4\", {\"data-testid\": \"listing-details__description-headline\"})\n",
    "    except:\n",
    "        property_metadata[property_url]['desc_title'] = None\n",
    "\n",
    "    # Scrape property description\n",
    "    try:\n",
    "        property_metadata[property_url]['desc'] = bs_object_rent \\\n",
    "            .find(\"div\", {\"data-testid\": \"listing-details__description\"}) \\\n",
    "            .findAll(\"p\")\n",
    "    except:\n",
    "        property_metadata[property_url]['desc'] = None\n",
    "    \n",
    "    # Scrape neighbourhood demographic insights\n",
    "    try:\n",
    "        property_metadata[property_url]['neighbourhood_insights'] = bs_object_rent \\\n",
    "            .findAll(\"tr\", {\"data-testid\": \"neighbourhood-insights__age-brackets-row\"})\n",
    "    except:\n",
    "        property_metadata[property_url]['neighbourhood_insights'] = None\n",
    "    \n",
    "    df = pd.DataFrame(property_metadata).transpose()\n",
    "    \n",
    "    \n",
    "    # Export df to csv, if is the first url in list then include index and headers\n",
    "    if url_links[0] == property_url:\n",
    "        df.to_csv(f'../data/raw/property_data{str(date.today())[4:]}.csv', mode='a', header=True, index=True)\n",
    "    else:\n",
    "        df.to_csv(f'../data/raw/property_data{str(date.today())[4:]}.csv', mode='a', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a462a22fa71c5522e4623dbf6c0f347b13b42bb86023cc012c310f1a6daec49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
